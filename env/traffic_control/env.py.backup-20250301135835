import gym
import numpy as np
import re
import copy
import os
import json
import logging
from typing import Optional, List, Tuple, Any, Dict, Union
from typing_extensions import override

from ragen.env.base import BaseDiscreteActionEnv
from .cityflow_adapter import CityFlowAdapter

class TrafficControlEnv(BaseDiscreteActionEnv):
    """
    Traffic Control Environment for traffic signal control.
    
    ## Description
    This environment simulates a traffic network with multiple intersections.
    The agent controls the traffic signals to optimize traffic flow.
    
    ## Action Space
    The action shape is `(1,)` in the range `{1, 2, 3, 4}` indicating
    which phase to set for the intersection:
    - 0: Invalid action
    - 1: Phase 1 (WSES - West-East Straight, East-South)
    - 2: Phase 2 (NSSS - North-South Straight, South-South)
    - 3: Phase 3 (WLEL - West-Left, East-Left)
    - 4: Phase 4 (NLSL - North-Left, South-Left)
    
    ## Observation
    Text-based observation describing the current state of each intersection:
    - Number of vehicles on each lane
    - Waiting vehicles
    - Current signal phase
    - Traffic pressure
    
    ## Rewards
    - Negative reward for traffic pressure (-0.25 * pressure)
    - Negative reward for queue length (-0.25 * queue_length)
    - Positive reward for throughput (0.5 * throughput)
    - Penalty for invalid actions (-1.0)
    """
    
    INVALID_ACTION = 0
    PENALTY_FOR_INVALID = -1.0
    
    # Define lookup for action names
    ACTION_LOOKUP = {
        0: "Invalid",
        1: "WSES (West-East Straight)",
        2: "NSSS (North-South Straight)",
        3: "WLEL (West-Left East-Left)",
        4: "NLSL (North-Left South-Left)",
    }
    
    # Define phase explanations for better understanding
    PHASE_EXPLANATIONS = {
        1: "WSES: Green light for vehicles going straight from West to East and East to West",
        2: "NSSS: Green light for vehicles going straight from North to South and South to North",
        3: "WLEL: Green light for vehicles turning left from West and East",
        4: "NLSL: Green light for vehicles turning left from North and South",
    }
    
    def __init__(self, **kwargs):
        """
        Initialize the Traffic Control Environment.
        
        Args:
            path_to_log: Path to log directory (default: './log')
            path_to_work_directory: Path to work directory (default: './data/traffic')
            dic_traffic_env_conf: Traffic environment configuration dictionary
            dic_path: Dictionary containing paths
            max_steps: Maximum number of steps (default: 3600)
            num_intersections: Number of intersections (default: 1)
        """
        BaseDiscreteActionEnv.__init__(self)
        
        # Set up logging
        logging.basicConfig(level=logging.INFO, 
                          format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        self.logger = logging.getLogger('TrafficControlEnv')
        
        # Extract parameters with defaults
        self.path_to_log = kwargs.get('path_to_log', './log')
        self.path_to_work_directory = kwargs.get('path_to_work_directory', './data/traffic')
        self.dic_traffic_env_conf = kwargs.get('dic_traffic_env_conf', {})
        self.dic_path = kwargs.get('dic_path', {})
        self.max_steps = kwargs.get('max_steps', 3600)
        self.num_intersections = kwargs.get('num_intersections', 1)
        
        # Ensure directories exist
        os.makedirs(self.path_to_log, exist_ok=True)
        os.makedirs(self.path_to_work_directory, exist_ok=True)
        
        # Initialize default configuration if not provided
        if not self.dic_traffic_env_conf:
            self.dic_traffic_env_conf = {
                'NUM_INTERSECTIONS': 1,
                'NUM_PHASES': 4,
                'PHASE': ["WSES", "NSSS", "WLEL", "NLSL"],
                'NUM_LANES': [3, 3, 3, 3],
                'YELLOW_TIME': 3,
                'MIN_ACTION_TIME': 10,
                'ROADNET_FILE': 'roadnet_1x2.json',
                'TRAFFIC_FILE': 'flow_1x2.json',
                'DIC_REWARD_INFO': {
                    'pressure': -0.25,
                    'queue_length': -0.25,
                    'throughput': 0.5
                }
            }
        
        # Initialize CityFlow adapter
        self.adapter = CityFlowAdapter(
            path_to_log=self.path_to_log,
            path_to_work_directory=self.path_to_work_directory,
            dic_traffic_env_conf=self.dic_traffic_env_conf,
            dic_path=self.dic_path
        )
        
        # Set up action space (discrete with 4 actions, starting from 1)
        self.num_phases = len(self.dic_traffic_env_conf.get('PHASE', ["WSES", "NSSS", "WLEL", "NLSL"]))
        self.ACTION_SPACE = gym.spaces.discrete.Discrete(self.num_phases, start=1)
        
        # Initialize tracking variables
        self.reward = 0
        self.step_count = 0
        self.current_state = None
        self.previous_actions = []
        self.cumulative_rewards = []
        self._reset_tracking_variables()

        self.logger.info(self.dic_traffic_env_conf)
        
        self.logger.info(f"TrafficControlEnv initialized with {self.num_intersections} intersections")
    
    def extract_action(self, text: str) -> int:
        """
        Extract action from text. Parse different formats of specifying phases.
        
        Args:
            text: Text containing the action
            
        Returns:
            int: Action code (1-4 for valid phases, 0 for invalid)
        """
        # Look for patterns like "Phase: 1", "Phase 1", "I choose signal phase: 1", etc.
        pattern = r'(?:phase|signal)(?:\s*:)?\s*([1-4])'
        match = re.search(pattern, text.lower())
        
        if match:
            return int(match.group(1))
        
        # Alternative: Check for phase names or abbreviations
        phase_patterns = {
            r'\bwses\b|\bwest.?east\s+straight\b': 1,
            r'\bnsss\b|\bnorth.?south\s+straight\b': 2,
            r'\bwlel\b|\bwest.?east\s+left\b': 3,
            r'\bnlsl\b|\bnorth.?south\s+left\b': 4
        }
        
        for pattern, action in phase_patterns.items():
            if re.search(pattern, text.lower()):
                return action
        
        # Final fallback: Check for direct numbers
        number_match = re.search(r'\b([1-4])\b', text)
        if number_match:
            return int(number_match.group(1))
        
        # No valid action found
        return self.INVALID_ACTION
    
    def reset(self, seed=None, mode='text'):
        """
        Reset the environment to initial state.
        
        Args:
            seed: Random seed for reproducibility
            mode: Rendering mode (text or rgb_array)
            
        Returns:
            str: Initial observation
        """
        self.logger.info(f"Resetting environment with seed={seed}")
        self._reset_tracking_variables()
        self.step_count = 0
        self.previous_actions = []
        self.cumulative_rewards = []
        
        # Reset the CityFlow adapter
        self.adapter.reset()
        
        # Get initial state
        self.current_state = self.adapter.get_state()
        
        # Render the initial state
        observation = self.render(mode)
        
        # Add initial prompt information
        observation = (
            "# Traffic Signal Control Task\n\n"
            "You are an intelligent traffic signal controller. Your task is to optimize "
            "traffic flow by selecting appropriate signal phases at each step.\n\n"
            "## Current Traffic State:\n" + observation + "\n\n"
            "## Available Signal Phases:\n"
            "1. WSES: Green for West-East Straight traffic\n"
            "2. NSSS: Green for North-South Straight traffic\n"
            "3. WLEL: Green for West-East Left-turn traffic\n"
            "4. NLSL: Green for North-South Left-turn traffic\n\n"
            "Analyze the traffic conditions and select the optimal signal phase (1-4)."
        )
        
        return observation
    
    def step(self, action: int) -> Tuple[Any, float, bool, Dict]:
        """
        Take a step in the environment with the given action.
        
        Args:
            action: Action to take (1-4 for different phases, 0 for invalid)
            
        Returns:
            observation: Text-based observation
            reward: Reward for the action
            done: Whether the episode is done
            info: Additional information
        """
        self.logger.debug(f"Step {self.step_count}: Action {action}")
        
        # Check if action is valid
        action_is_valid = action in self.get_all_actions()
        
        # Apply action to the environment
        if action_is_valid:
            # Convert action to 0-indexed for the adapter (CityFlow uses 0-indexed)
            adapter_action = action - 1
            self.adapter.step(adapter_action)
            self.current_state = self.adapter.get_state()
            
            # Calculate reward based on traffic metrics
            reward = self._calculate_reward()
            action_is_effective = True
            
            # Store previous action for history
            self.previous_actions.append(action)
        else:
            # Invalid action
            reward = self.PENALTY_FOR_INVALID
            action_is_effective = False
            self.previous_actions.append(self.INVALID_ACTION)
        
        # Track cumulative reward
        self.cumulative_rewards.append(reward)
        
        # Update step count
        self.step_count += 1
        
        # Check if episode is done
        done = self.finished()
        
        # Render the new state
        observation = self.render()
        
        # Add action history and analysis to observation
        observation = self._enhance_observation(observation, action, reward)
        
        # Update tracking variables
        self._update_tracking_variables(
            response=str(action),
            action=action,
            action_is_valid=action_is_valid,
            action_is_effective=action_is_effective,
            reward=reward
        )
        
        # Prepare info dictionary
        info = {
            "action_is_valid": action_is_valid,
            "action_is_effective": action_is_effective,
            "step_count": self.step_count,
            "total_reward": self.reward,
            "current_reward": reward
        }
        
        return observation, reward, done, info
    
    def _enhance_observation(self, observation: str, action: int, reward: float) -> str:
        """
        Enhance the observation with additional useful information.
        
        Args:
            observation: Raw observation string
            action: Last action taken
            reward: Reward received
            
        Returns:
            str: Enhanced observation
        """
        # Add action history
        action_history = "## Recent Actions:\n"
        for i, a in enumerate(self.previous_actions[-5:]):
            step_num = self.step_count - len(self.previous_actions[-5:]) + i
            action_name = self.ACTION_LOOKUP.get(a, "Unknown")
            action_history += f"Step {step_num}: {action_name}\n"
        
        # Add reward information
        reward_info = (
            f"## Reward Information:\n"
            f"Last Reward: {reward:.2f}\n"
            f"Cumulative Reward: {self.reward:.2f}\n"
        )
        
        # Add guidance for reasoning
        reasoning_guide = (
            f"## Analysis Guidance:\n"
            f"1. Consider the number of vehicles in each direction\n"
            f"2. Pay attention to waiting vehicles (stopped at intersections)\n"
            f"3. Try to reduce traffic pressure and queue length\n"
            f"4. Check which directions have the most congestion\n"
            f"5. Consider the impact of your decision on future traffic flow\n\n"
            f"Based on your analysis, select the optimal signal phase (1-4)."
        )
        
        return f"{observation}\n\n{action_history}\n{reward_info}\n{reasoning_guide}"
    
    def _calculate_reward(self) -> float:
        """
        Calculate reward based on traffic metrics.
        
        Returns:
            float: Reward value
        """
        if not self.current_state:
            return 0.0
        
        # Extract reward components from config
        reward_info = self.dic_traffic_env_conf.get('DIC_REWARD_INFO', {
            'pressure': -0.25,
            'queue_length': -0.25,
            'throughput': 0.5
        })
        
        # Initialize metrics
        total_pressure = 0
        total_queue_length = 0
        total_throughput = 0
        
        # Calculate metrics for each intersection
        for intersection_id in range(self.num_intersections):
            if intersection_id in self.current_state:
                intersection_state = self.current_state[intersection_id]
                
                # Traffic pressure (difference between incoming and outgoing vehicles)
                if 'pressure' in intersection_state:
                    total_pressure += abs(intersection_state['pressure'])
                
                # Queue length (vehicles waiting at the intersection)
                if 'lane_queue' in intersection_state:
                    total_queue_length += sum(intersection_state['lane_queue'])
                elif 'lane_waiting_vehicle_count' in intersection_state:
                    total_queue_length += sum(intersection_state['lane_waiting_vehicle_count'])
                
                # Throughput (vehicles that successfully passed through)
                if 'throughput' in intersection_state:
                    total_throughput += intersection_state['throughput']
                # Estimate throughput if not directly available
                elif 'lane_vehicle_count' in intersection_state and len(self.previous_actions) > 0:
                    current_count = sum(intersection_state['lane_vehicle_count'])
                    # Check if we have previous state to calculate how many vehicles passed
                    if hasattr(self, '_prev_vehicle_count'):
                        # Vehicles that entered minus vehicles that exited
                        throughput = max(0, self._prev_vehicle_count - current_count + 2)  # Adding constant for vehicles that entered
                        total_throughput += throughput
                    self._prev_vehicle_count = current_count
        
        # Calculate reward components
        pressure_reward = reward_info.get('pressure', -0.25) * total_pressure
        queue_reward = reward_info.get('queue_length', -0.25) * total_queue_length
        throughput_reward = reward_info.get('throughput', 0.5) * total_throughput
        
        # Combine rewards
        total_reward = pressure_reward + queue_reward + throughput_reward
        
        # Log detailed reward breakdown
        self.logger.debug(f"Reward breakdown - Pressure: {pressure_reward:.2f}, "
                        f"Queue: {queue_reward:.2f}, Throughput: {throughput_reward:.2f}, "
                        f"Total: {total_reward:.2f}")
        
        return total_reward
    
    def finished(self) -> bool:
        """
        Check if the episode is finished.
        
        Returns:
            bool: True if the episode is finished, False otherwise
        """
        return self.step_count >= self.max_steps
    
    def success(self) -> bool:
        """
        Check if the episode was successful.
        In traffic control, success is measured by maintaining good traffic flow.
        
        Returns:
            bool: True if the episode was successful, False otherwise
        """
        # Define success as having an average reward above a threshold
        if not self.cumulative_rewards:
            return False
        
        avg_reward = sum(self.cumulative_rewards) / len(self.cumulative_rewards)
        success_threshold = -5.0  # This threshold can be adjusted
        
        return avg_reward > success_threshold
    
    def render(self, mode='text') -> str:
        """
        Render the environment as a text representation.
        
        Args:
            mode: Rendering mode ('text' or 'rgb_array')
            
        Returns:
            str: Text representation of the environment state
        """
        if not self.current_state:
            return "No state information available."
        
        if mode == 'rgb_array':
            # This would return an image representation if implemented
            # For now, we'll just return the text representation
            return self.render(mode='text')
        
        # Build a text representation of the traffic state
        text = "Traffic State Summary:\n"
        text += f"Step: {self.step_count}/{self.max_steps}\n\n"
        
        # Process each intersection
        for intersection_id in range(self.num_intersections):
            if intersection_id in self.current_state:
                intersection_state = self.current_state[intersection_id]
                
                text += f"Intersection {intersection_id + 1}:\n"
                text += "------------------------\n"
                
                # Current phase
                if 'current_phase' in intersection_state:
                    phase_idx = intersection_state['current_phase']
                    phase_name = self.ACTION_LOOKUP.get(phase_idx + 1, "Unknown")
                    text += f"Current Phase: {phase_name}\n"
                
                # Traffic pressure
                if 'pressure' in intersection_state:
                    text += f"Traffic Pressure: {intersection_state['pressure']:.2f}\n"
                
                # Vehicle counts by approach direction
                if 'lane_vehicle_count' in intersection_state:
                    counts = intersection_state['lane_vehicle_count']
                    directions = ["North", "East", "South", "West"]
                    lanes_per_direction = len(counts) // 4
                    
                    text += "\nVehicle Counts by Direction:\n"
                    for i, direction in enumerate(directions):
                        start_idx = i * lanes_per_direction
                        end_idx = start_idx + lanes_per_direction
                        direction_counts = counts[start_idx:end_idx]
                        total = sum(direction_counts)
                        text += f"  {direction}: {total} vehicles\n"
                
                # Waiting vehicles
                if 'lane_waiting_vehicle_count' in intersection_state:
                    waiting_counts = intersection_state['lane_waiting_vehicle_count']
                    
                    text += "\nWaiting Vehicles by Direction:\n"
                    for i, direction in enumerate(directions):
                        start_idx = i * lanes_per_direction
                        end_idx = start_idx + lanes_per_direction
                        direction_waiting = waiting_counts[start_idx:end_idx]
                        total_waiting = sum(direction_waiting)
                        text += f"  {direction}: {total_waiting} waiting\n"
                
                # Lane details
                text += "\nDetailed Lane Information:\n"
                if 'lane_vehicle_count' in intersection_state and 'lane_waiting_vehicle_count' in intersection_state:
                    counts = intersection_state['lane_vehicle_count']
                    waiting = intersection_state['lane_waiting_vehicle_count']
                    
                    for i, (count, wait) in enumerate(zip(counts, waiting)):
                        direction = directions[i // lanes_per_direction]
                        lane = i % lanes_per_direction + 1
                        text += f"  {direction} Lane {lane}: {count} vehicles ({wait} waiting)\n"
                
                text += "\n"
        
        return text
    
    def copy(self) -> 'TrafficControlEnv':
        """
        Create a deep copy of the environment.
        
        Returns:
            TrafficControlEnv: A deep copy of the environment
        """
        new_env = TrafficControlEnv(
            path_to_log=self.path_to_log,
            path_to_work_directory=self.path_to_work_directory,
            dic_traffic_env_conf=copy.deepcopy(self.dic_traffic_env_conf),
            dic_path=copy.deepcopy(self.dic_path),
            max_steps=self.max_steps,
            num_intersections=self.num_intersections
        )
        
        # Copy current state
        if self.current_state is not None:
            new_env.current_state = copy.deepcopy(self.current_state)
        
        # Copy tracking variables
        new_env.step_count = self.step_count
        new_env.previous_actions = copy.deepcopy(self.previous_actions)
        new_env.cumulative_rewards = copy.deepcopy(self.cumulative_rewards)
        new_env._copy_tracking_variables(self)
        
        if hasattr(self, '_prev_vehicle_count'):
            new_env._prev_vehicle_count = self._prev_vehicle_count
        
        return new_env
    
    def get_all_actions(self) -> List[int]:
        """
        Get all valid actions.
        
        Returns:
            List[int]: List of valid actions
        """
        return list(range(1, self.num_phases + 1))
    
    def _reset_tracking_variables(self):
        """Reset tracking variables."""
        super()._reset_tracking_variables()
        self.step_count = 0
        self.previous_actions = []
        self.cumulative_rewards = []
        if hasattr(self, '_prev_vehicle_count'):
            delattr(self, '_prev_vehicle_count')
    
    def compute_score(self, solution_str: str, ground_truth: str) -> float:
        """
        Compute the score for the traffic control task.
        
        Args:
            solution_str: The solution string from the model
        """
        pass
    


# Traffic Control Guide for reference
GUIDE = """
### Traffic Signal Control Instructions

In this task, you are controlling traffic signals at intersections to optimize traffic flow. 
Your goal is to reduce congestion, minimize waiting times, and improve overall traffic efficiency.

---

#### Signal Phases and Their Meaning
- **WSES (Phase 1)**: West-East Straight traffic gets green light
- **NSSS (Phase 2)**: North-South Straight traffic gets green light
- **WLEL (Phase 3)**: West-East Left-turn traffic gets green light
- **NLSL (Phase 4)**: North-South Left-turn traffic gets green light

---

#### Your Goal
Select the optimal signal phase at each step to minimize traffic congestion, reduce waiting times,
and maximize throughput of vehicles through the intersection.

---

#### Traffic Metrics
- **Traffic Pressure**: Imbalance between incoming and outgoing traffic
- **Queue Length**: Number of vehicles waiting at the intersection
- **Waiting Vehicles**: Vehicles that are stopped (speed near zero)
- **Throughput**: Number of vehicles successfully passing through the intersection

---

#### Controls
Use these outputs to select a signal phase:
- `1`: Set signal to **WSES** (West-East Straight)
- `2`: Set signal to **NSSS** (North-South Straight)
- `3`: Set signal to **WLEL** (West-East Left-turn)
- `4`: Set signal to **NLSL** (North-South Left-turn)

---

#### Rewards
- **Traffic Pressure**: Each unit of traffic pressure costs -0.25
- **Queue Length**: Each waiting vehicle costs -0.25
- **Throughput**: Each vehicle passing through gives +0.5
- **Invalid Action**: Selecting an invalid action costs -1.0

---

Enjoy optimizing traffic flow!
"""

if __name__ == '__main__':
    # Test the environment
    env = TrafficControlEnv()
    observation = env.reset(seed=42)
    print(observation)
    
    # Take a few random steps
    for i in range(10):
        action = np.random.randint(1, 5)  # Random action between 1 and 4
        observation, reward, done, info = env.step(action)
        print(f"\nStep {i+1}, Action: {action}, Reward: {reward:.2f}")
        print(observation)
        if done:
            break
    
    print("\nEnvironment test complete.")